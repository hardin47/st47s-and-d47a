{
  "hash": "b9d081cf0637f781c2f9b12b3a4b6f90",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Algorithmic Bias\nauthor: Jo Hardin\ndate: '2025-07-07'\nslug: algbias\nfeatured: true\ncategories: []\ntags: []\nimage: featured.png\ndescription: \"Model bias can come from any of a plethora of places. Sometimes it is the training data which are problematic: sampling bias, measurment bias, historical bias, label bias. Sometimes there are problems in the modeling step itself: optimizing for particular criterion, overfitting or underfitting, choice of algorithm. Sometimes the problem can occur while data processing. For example, like here, different feature distributions for variables or imblance in classes.\"\nexecute:\n  echo: false\n  warning: false\n  message: false\nbibliography: algbias.bib\n---\n\n\n\n\n\n:::{.blog}\n::::{.blog-header}\nAlgorithmic Unfairness Without Any Bias Baked In by Aaron Roth\n::::\n\n::::{.blog-container}\n\nThe following entry is based heavily on a blog by Aaron Roth <a href = \"http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html\" target = \"_blank\">Algorithmic Unfairness Without Any Bias Baked In</a>.\n\n::::\n:::\n \n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n<br/>\n\n\nConsider an example taken directly (and mostly verbatim) from a blog by Aaron Roth [Algorithmic Unfairness Without Any Bias Baked In](http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html). We describe how forced \"fairness\" can have unintentional consequences.\n\n## Set up of the model\n\nIn the population, we have only one type of person. They have the same `talent`, `grades`, and `SAT` scores.\n\n```\ntalent ~ Normal (100, 15)\ngrades ~ Normal (talent, 15)\nSAT ~ Normal (talent, 15)\n```\n\nA particular college wants to admit students with \n\n> `talent > 115` \n\n... but they only have access to `grades` and `SAT` which are noisy estimates of `talent`.\n\n\nThe college sets up a plan for accepting students. Fortunately for the college, they have training data which is a true random sample from the population (and includes all three variablees: `talent`, `grades`, and `SAT`). There is no bias in the selection of the training data from the population.\n\nIn order to create a model which will help the college decide who to admit, they follow the following steps:\n\n1. Run a regression on a training dataset (`talent` is known for existing students)\n2. Find a model which predicts `talent` based on `grades` and `SAT`\n3. Choose students for whom predicted `talent` is above 115 (for the students who apply to the college)\n\n\n## The dilemma\n\nUnfortunately for the college, there is a flaw in their admission plan. As mentioned previously, there is only one population of students when it comes to their inherent abilities.\n\n* there are two populations of students, the Reds and Blues. \n* Reds are the majority population (99%), and Blues are a small minority population (1%) \n* the Reds and the Blues are no different when it comes to talent: they both have the same talent distribution, as described above. \n* there is no bias baked into the grading or the exams: both the Reds and the Blues also have exactly the same grade and exam score distributions\n\n\nHowever, there is one very meaningful difference in the two populations.\n\n> The Blues have more money than the Reds, so they each take the **SAT twice**, and report only the highest of the two scores to the college. This results in a small but noticeable bump in their average SAT scores, compared to the Reds.\n\nLet's underscore the problem here:\n\n> The value of `SAT` means something different for the Reds versus the Blues\n\nAnother way to describe the problem is to say that the two groups of students have different feature distributions.\n\n\n## What happens?\n\nWe start by visualizing the `grades` and `SAT` scores of the two populations. We can see that, as expected, the Blues have slightly higher `SAT` scores.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n### Fitting two models, separately\n\nIf we fit the data to the Reds separately from the Blues, we can come up with two separate linear models, one for each population. \n\n\n\n::: {.cell}\n\n:::\n\n\n\nRed model (SAT taken once):\n\n\n::: {.cell}\n\n```\n## # A tibble: 3 × 5\n##   term        estimate std.error statistic p.value\n##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept)   33.2     0.152        218.       0\n## 2 SAT            0.334   0.00149      224.       0\n## 3 grades         0.334   0.00149      225.       0\n```\n:::\n\n\nBlue model (SAT is max score of two):\n\n\n::: {.cell}\n\n```\n## # A tibble: 3 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)   25.3      1.60        15.8 2.04e- 50\n## 2 SAT            0.432    0.0161      26.7 3.35e-119\n## 3 grades         0.277    0.0154      18.0 6.83e- 63\n```\n:::\n\n\n\n\n\nUsing the separate linear models, we generate new data, some Red and some Blue, and measure whether or not the models are biased. We want to know how well the models will be able to predict if a student has `talent` > 115.\n\nFrom the plot below, on new students, we do not see any obvious bias. That is, the Blue points seem to be just as likely to be predicted to be a false positive as the Red points. Similarly, the Blue points seem to be just as likely to be predicted to be a false negative as the Red points.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\nWe can quantify the error rates across the Blue and Red groups.\n\n> tpr = talent > 115 & predicted > 115 / talent > 115  \n> fpr = talent < 115 & predicted > 115 / talent < 115  \n> fnr = talent > 115 & predicted < 115 / talent > 115  \n> error = fp + fn / total\n\n\n\n\n::: {.cell}\n\n```\n## # A tibble: 2 × 5\n##   color   tpr    fpr   fnr error\n##   <chr> <dbl>  <dbl> <dbl> <dbl>\n## 1 Blue  0.503 0.0379 0.497 0.109\n## 2 Red   0.509 0.0378 0.491 0.109\n```\n:::\n\n\n\n#### Can we fit two models?\n\n\nFitting **two** models doesn't seem right? After all, there are laws against using protected classes to make decisions for housing, jobs, money loans, college, etc.\n\n### Fitting one model\n\nWhat happens if we use the training data to come up with a single model and use it to predict for both the Blues and the Reds?\n\n\n\n::: {.cell}\n\n```\n## # A tibble: 3 × 5\n##   term        estimate std.error statistic p.value\n##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept)   33.1     0.151        219.       0\n## 2 SAT            0.334   0.00148      225.       0\n## 3 grades         0.334   0.00148      226.       0\n```\n:::\n\n\n\nWe notice that the coefficients look like the Red model. That's because Reds are the majority class, and so their data dominate the training data.\n\n\nUsing one model, we can assess how the error rates change.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\nOne model:\n\n\n::: {.cell}\n\n```\n## # A tibble: 2 × 5\n##   color   tpr    fpr   fnr error\n##   <chr> <dbl>  <dbl> <dbl> <dbl>\n## 1 Blue  0.619 0.0627 0.381 0.112\n## 2 Red   0.507 0.0375 0.493 0.109\n```\n:::\n\n\n\nTwo separate models:\n\n\n::: {.cell}\n\n```\n## # A tibble: 2 × 5\n##   color   tpr    fpr   fnr error\n##   <chr> <dbl>  <dbl> <dbl> <dbl>\n## 1 Blue  0.503 0.0379 0.497 0.109\n## 2 Red   0.509 0.0378 0.491 0.109\n```\n:::\n\n\n\n\n\n##  What did we learn?\n\n> with two populations that have different feature distributions, learning a single classifier (that is prohibited from discriminating based on population) will fit the bigger of the two populations\n\n* depending on the nature of the distribution difference, it can be either to the benefit or the detriment of the minority population\n\n* no explicit human bias, either on the part of the algorithm designer or the data gathering process\n\n* the problem is exacerbated if we artificially force the algorithm to be group blind\n\n* well-intentioned \"fairness\" regulations prohibiting decision makers form taking sensitive attributes into account can actually make things less fair and less accurate at the same time\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}