---
title: Algorithmic Bias
author: Jo Hardin
date: '2025-07-07'
slug: algbias
featured: true
categories: []
tags: []
image: featured.png
description: "Model bias can come from any of a plethora of places. Sometimes it is the training data which are problematic: sampling bias, measurment bias, historical bias, label bias. Sometimes there are problems in the modeling step itself: optimizing for particular criterion, overfitting or underfitting, choice of algorithm. Sometimes the problem can occur while data processing. For example, like here, different feature distributions for variables and imblance in classes."
execute:
  echo: false
  warning: false
  message: false
bibliography: algbias.bib
---



:::{.blog}
::::{.blog-header}
Algorithmic Unfairness Without Any Bias Baked In by Aaron Roth
::::

::::{.blog-container}

The following entry is based heavily on a blog by Aaron Roth <a href = "http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html" target = "_blank">Algorithmic Unfairness Without Any Bias Baked In</a>.

::::
:::
 

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```


```{r}
#| echo: false

library(tidyverse)
```


<br/>


Consider an example taken directly (and mostly verbatim) from a blog by Aaron Roth [Algorithmic Unfairness Without Any Bias Baked In](http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html) [@roth2019]. We describe how forced "fairness" can have unintentional consequences.

## Set up of the model

In the population, we have only one type of person. They have the same `talent`, `grades`, and `SAT` scores.

```
talent ~ Normal (100, 15)
grades ~ Normal (talent, 15)
SAT ~ Normal (talent, 15)
```

A particular college wants to admit students with 

> `talent > 115` 

... but they only have access to `grades` and `SAT` which are noisy estimates of `talent`.


The college sets up a plan for accepting students. Fortunately for the college, they have training data which is a true random sample from the population (and includes all three variables: `talent`, `grades`, and `SAT`). There is no bias in the selection of the training data from the population.

In order to create a model which will help the college decide who to admit, they follow the following steps:

1. Run a regression on a training dataset (`talent` is known for existing students)
2. Find a model which predicts `talent` based on `grades` and `SAT`
3. Choose students for whom predicted `talent` is above 115 (for the students who apply to the college)


## The dilemma

Unfortunately for the college, there is a flaw in their admission plan. As mentioned previously, there is only one population of students when it comes to their inherent abilities.

* there are two populations of students, the Reds and Blues. 
* Reds are the majority population (99%), and Blues are a small minority population (1%) 
* the Reds and the Blues are no different when it comes to talent: they both have the same talent distribution, as described above. 
* there is no bias baked into the grading or the exams: both the Reds and the Blues also have exactly the same grade and exam score distributions


However, there is one very meaningful difference in the two populations.

> The Blues have more money than the Reds, so they each take the **SAT twice**, and report only the highest of the two scores to the college. This results in a small but noticeable bump in their average SAT scores, compared to the Reds.

Let's underscore the problem here:

> The value of `SAT` means something different for the Reds versus the Blues

Another way to describe the problem is to say that the two groups of students have different feature distributions.


## What happens?

We start by visualizing the `grades` and `SAT` scores of the two populations. We can see that, as expected, the Blues have slightly higher `SAT` scores.

```{r echo=FALSE}
set.seed(470)
n_obs <- 100000
n.red <- n_obs*0.99
n.blue <- n_obs*0.01
reds <- rnorm(n.red, mean = 100, sd = 15)
blues <- rnorm(n.blue, mean = 100, sd = 15)

red.sat <- reds + rnorm(n.red, mean = 0, sd = 15)
blue.sat <- blues + 
    pmax(rnorm(n.blue, mean = 0, sd = 15),
         rnorm(n.blue, mean = 0, sd = 15))

red.grade <- reds + rnorm(n.red, mean = 0, sd = 15)
blue.grade <- blues + rnorm(n.blue, mean = 0, sd = 15)

college.data <- data.frame(talent = c(reds, blues),
                           SAT = c(red.sat, blue.sat),
                           grades = c(red.grade, blue.grade),
                           color = c(rep("Red", n.red), rep("Blue", n.blue)))

ggplot(college.data, aes(x = grades, y = SAT, color = color)) +
  geom_point(size = 0.5) +
  scale_color_identity(name = "Color Group",
                       guide = "legend") +
  geom_abline(intercept = 0, slope = 1)
  
```


### Fitting two models, separately

If we fit the data to the Reds separately from the Blues, we can come up with two separate linear models, one for each population. 

```{r echo = FALSE}
red.lm = college.data |>
  dplyr::filter(color == "Red") %>%
  lm(talent ~ SAT + grades, data = .)

blue.lm = college.data |>
  dplyr::filter(color == "Blue") %>%
  lm(talent ~ SAT + grades, data = .)
```

Red model (SAT taken once):
```{r echo = FALSE}
red.lm |> broom::tidy()
```
Blue model (SAT is max score of two):
```{r echo = FALSE}
blue.lm |> broom::tidy()
```



Using the separate linear models, we generate new data, some Red and some Blue, and measure whether or not the models are biased. We want to know how well the models will be able to predict if a student has `talent` > 115.

From the plot below, on new students, we do not see any obvious bias. That is, the Blue points seem to be just as likely to be predicted to be a false positive as the Red points. Similarly, the Blue points seem to be just as likely to be predicted to be a false negative as the Red points.

```{r echo=FALSE}
set.seed(4747)
new.reds <- rnorm(n.red, mean = 100, sd = 15)
new.blues <- rnorm(n.blue, mean = 100, sd = 15)

new.red.sat <- new.reds + rnorm(n.red, mean = 0, sd = 15)
new.blue.sat <- new.blues + 
    pmax(rnorm(n.blue, mean = 0, sd = 15),
         rnorm(n.blue, mean = 0, sd = 15))

new.red.grade <- new.reds + rnorm(n.red, mean = 0, sd = 15)
new.blue.grade <- new.blues + rnorm(n.blue, mean = 0, sd = 15)

new.college.data <- data.frame(talent = c(new.reds, new.blues),
                           SAT = c(new.red.sat, new.blue.sat),
                           grades = c(new.red.grade, new.blue.grade),
                           color = c(rep("Red", n.red), rep("Blue", n.blue)))


new.red.pred <- new.college.data |>
  filter(color == "Red") %>%
  predict.lm(red.lm, newdata = .)

new.blue.pred <- new.college.data |>
  filter(color == "Blue") %>%
  predict.lm(blue.lm, newdata = .)

new.college.data <- new.college.data |> 
  cbind(predicted = c(new.red.pred, new.blue.pred))

ggplot(new.college.data, aes(x = talent, y = predicted, color = color)) + 
  geom_point(size = 0.5) + 
  geom_hline(yintercept = 115) + 
  geom_vline(xintercept = 115) +
  scale_color_identity(name = "Color Group",
                       guide = "legend") + 
  labs(y = "prediction of talent - separate models",
       x = "original observed talent") + 
  geom_abline(slope = 1, intercept = 0, color = "grey")
```


We can quantify the error rates across the Blue and Red groups.

> tpr = talent > 115 & predicted > 115 / talent > 115  
> fpr = talent < 115 & predicted > 115 / talent < 115  
> fnr = talent > 115 & predicted < 115 / talent > 115  
> error = fp + fn / total


```{r echo=FALSE}
new.college.data <- new.college.data |> 
  mutate(fp = ifelse(talent < 115 & predicted > 115, 1, 0),
         tp = ifelse(talent > 115 & predicted > 115, 1, 0),
         fn = ifelse(talent > 115 & predicted < 115, 1, 0),
         tn = ifelse(talent < 115 & predicted < 115, 1, 0))

error.rates <- new.college.data |> group_by(color) |>
  summarize(tpr = sum(tp) / (sum(tp) + sum(fn)),
            fpr = sum(fp) / (sum(fp) + sum(tn)),
            fnr = sum(fn) / (sum(fn) + sum(tp)),
            #fdr = sum(fp) / (sum(fp) + sum(tp)),
            error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) ))

error.rates
```

#### Can we fit two models?


Fitting **two** models doesn't seem right? After all, there are laws against using protected classes to make decisions for housing, jobs, money loans, college, etc.

### Fitting one model

What happens if we use the training data to come up with a single model and use it to predict for both the Blues and the Reds?

```{r echo = FALSE}
global.lm = college.data %>%
  lm(talent ~ SAT + grades, data = .)

global.lm |> broom::tidy()
```

We notice that the coefficients look like the Red model. That's because Reds are the majority class, and so their data dominate the training data.


Using one model, we can assess how the error rates change. Already, the plot shows that the Blue points are no longer centered in the middle of the Red points. The Blue points are predicted 

```{r echo=FALSE}
new.pred <- new.college.data %>%
  predict.lm(global.lm, newdata = .)

new.college.data <- new.college.data |> 
  cbind(global.predicted = new.pred)

ggplot(new.college.data, aes(x = talent, 
                             y = global.predicted, 
                             color = color)) + 
  geom_point(size = 0.5) + 
  geom_hline(yintercept = 115) + 
  geom_vline(xintercept = 115) +
  scale_color_identity(name = "Color Group",
                       guide = "legend") + 
  labs(y = "prediction of talent - one model",
       x = "original observed talent") + 
  geom_abline(slope = 1, intercept = 0, color = "grey")
```


One model:
```{r echo = FALSE}
new.college.data.glb <- new.college.data |> 
  mutate(fp = ifelse(talent < 115 & global.predicted > 115, 1, 0),
         tp = ifelse(talent > 115 & global.predicted > 115, 1, 0),
         fn = ifelse(talent > 115 & global.predicted < 115, 1, 0),
         tn = ifelse(talent < 115 & global.predicted < 115, 1, 0))

error.rates <- new.college.data.glb |> group_by(color) |>
  summarize(tpr = sum(tp) / (sum(tp) + sum(fn)),
            fpr = sum(fp) / (sum(fp) + sum(tn)),
            fnr = sum(fn) / (sum(fn) + sum(tp)),
            #fdr = sum(fp) / (sum(fp) + sum(tp)),
            error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) ))

error.rates
```

Two separate models:
```{r echo = FALSE}
new.college.data.2mod <- new.college.data |> 
  mutate(fp = ifelse(talent < 115 & predicted > 115, 1, 0),
         tp = ifelse(talent > 115 & predicted > 115, 1, 0),
         fn = ifelse(talent > 115 & predicted < 115, 1, 0),
         tn = ifelse(talent < 115 & predicted < 115, 1, 0))

error.rates <- new.college.data.2mod |> group_by(color) |>
  summarize(tpr = sum(tp) / (sum(tp) + sum(fn)),
            fpr = sum(fp) / (sum(fp) + sum(tn)),
            fnr = sum(fn) / (sum(fn) + sum(tp)),
            #fdr = sum(fp) / (sum(fp) + sum(tp)),
            error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) ))

error.rates
```



##  What did we learn?

> with two populations that have different feature distributions, learning a single classifier (that is prohibited from discriminating based on population) will fit the bigger of the two populations

* depending on the nature of the distribution difference, it can be either to the benefit or the detriment of the minority population

* no explicit human bias, either on the part of the algorithm designer or the data gathering process

* the problem is exacerbated if we artificially force the algorithm to be group blind

* well-intentioned "fairness" regulations prohibiting decision makers form taking sensitive attributes into account can actually make things less fair and less accurate at the same time


## Simulation

Only one dataset under one set of conditions has been simulated in the example. To fully understand the extent of the bias, we would need to do a larger simulation which might include:

* repeat analysis to measure the variability of the error rates.
* change the number of times a student can take the `SAT`
* change the proportion of the population which is Blue
* change the original relationship -- `talent` can be more / less dependent on `grades` than `SAT`

What else would you like to know about the data, model, and simulation?

